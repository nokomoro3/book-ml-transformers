{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xooQEdfrjo5d"
      },
      "source": [
        "# Transformerの詳細\n",
        "\n",
        "- この章では、Transformerの詳細な実装について記載されている。\n",
        "- 実際にPyTorchを使って実装を進めていく"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwPms4OPjvyI"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "%cd notebooks\n",
        "from install import *\n",
        "install_requirements()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCZNf4f4jo5g"
      },
      "source": [
        "## 3.1 Transformerのアーキテクチャ\n",
        "\n",
        "- オリジナルのTransformerはエンコーダ・デコーダアーキテクチャとなっている。\n",
        "- もともと機械翻訳向けにこの構造をしている。\n",
        "\n",
        "<img src=\"https://github.com/nokomoro3/book-ml-transformers/blob/b2c6b3672a0de61117eba41f17e95002f317077f/img/ml-transformers-chap03-transformer-anatomy_2022-08-25-22-02-10.png?raw=1\" />\n",
        "\n",
        "- エンコーダの特徴\n",
        "  - 入力トークン系列を埋め込みベクトルの系列に変換する。\n",
        "  - 埋め込みベクトルは、隠れ状態やコンテキストとも呼ばれる。\n",
        "- デコーダの特徴\n",
        "  - 埋め込みベクトルの系列を入力し、出力トークン系列を生成する。\n",
        "  - デコーダの終了は、特別なEOSトークンに到達するまで継続される。\n",
        "\n",
        "- その後エンコーダ・デコーダのそれぞれが独立したモデルとして適応されていくこととなる。\n",
        "\n",
        "- エンコーダのみモデル\n",
        "  - テキスト分類や固有表現認識といったタスクに使用される。\n",
        "  - このアーキテクチャでは、与えられたあるトークンの結果が、前後双方のコンテキストに依存する。\n",
        "  - これは双方向アテンションと呼ばれ、BERT系が該当する。\n",
        "- デコーダのみモデル\n",
        "  - 次の単語を文脈から予測するようなタスクに使用される。\n",
        "  - このアーキテクチャでは、与えられたあるトークンの結果が、前方のコンテキストのみに依存する。\n",
        "  - これは因果的もしくは自己回帰型アテンションと呼ばれ、GPT系が該当する。\n",
        "- エンコーダ・デコーダモデル\n",
        "  - 機械翻訳や要約といったタスクに使用される。\n",
        "  - BARTやT5がこのアーキテクチャに該当する。\n",
        "\n",
        "- 実際にはこれらの区別はあいまいであるので注意が必要。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9k7hO9ujo5h"
      },
      "source": [
        "## 3.2 エンコーダ\n",
        "\n",
        "- Transformerのエンコーダは複数のエンコーダをスタックする。\n",
        "- エンコーダは文脈情報を埋め込んだ表現を生成する。\n",
        "- エンコーダは、マルチヘッドセルフアテンションと単純な順伝搬層で構成される。\n",
        "\n",
        "<img src=\"https://github.com/nokomoro3/book-ml-transformers/blob/b2c6b3672a0de61117eba41f17e95002f317077f/img/ml-transformers-chap03-transformer-anatomy_2022-08-25-22-14-41.png?raw=1\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS53mRJHjo5i"
      },
      "source": [
        "### 3.2.1 セルフアテンション\n",
        "\n",
        "- アテンションは系列の各要素に異なる重みを割り当てる機構である。\n",
        "- テキストの場合、系列の要素はトークン埋め込みである。\n",
        "- トークン埋め込みは、固定次元のベクトルで表される。\n",
        "  - 例えばBERTの場合、768次元のベクトルとなる。\n",
        "- セルフアテンションは、入力されるトークン埋め込み$x_j$すべてを使用した線形和を、その系列ぶん計算する。\n",
        "\n",
        "$$ x^{'}_i = \\sum^n_{j=1}{w_{ji}x_j}$$\n",
        "\n",
        "- $w_{ji}$はアテンションの重みと呼ばれ、$\\sum_{j}{w_{ji}}=1$となるように正規化される。\n",
        "- セルフアテンションは以下のように、同一単語であっても周囲の文脈を考慮した埋め込みを生成することができる。\n",
        "\n",
        "<img src=\"https://github.com/nokomoro3/book-ml-transformers/blob/b2c6b3672a0de61117eba41f17e95002f317077f/img/ml-transformers-chap03-transformer-anatomy_2022-08-26-09-09-37.png?raw=1\" />\n",
        "\n",
        "- この出力される結果を文脈埋め込みと呼び、Transformerに選考してELMoなどの言語モデルで取り入れられている。\n",
        "  - [Deep Contextualized Word Representations (2018-02-15)](https://arxiv.org/abs/1802.05365)\n",
        "- 以降は$w_{ji}$を計算する方法について述べる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFkaxgfljo5i"
      },
      "source": [
        "#### 3.2.1.1 スケール化ドット積アテンション\n",
        "\n",
        "- いくつか実装があるがTransformerでは以下のステップを踏む。\n",
        "  - 各トークン埋め込みをクエリ・キー・バリューというベクトルに射影する。\n",
        "  - クエリとキーの類似度を計算し、これをアテンションスコアと呼ぶ。\n",
        "    - アテンションスコアは、系列長をnとすると、n x n行列となる。\n",
        "  - アテンションスコアは任意の数を生成できるため、softmaxで正規化して、これを重み$ w_{ji} $とする。\n",
        "    - 正規化は入力系列方向に行い、$ \\sum_{j}{w_{ji}}=1 $となるようにする\n",
        "  - バリューベクトル$ v_1,...,v_n $の重み付け線形和$ x^{'}_i = \\sum^n_{j=1}{w_{ji}v_j} $で文脈埋め込みを計算する\n",
        "- これらの可視化をJupyter向けのBertVizにおけるneuron_viewで行うことができる。\n",
        "  - layerはエンコーダスタックの番号、headは後述するマルチヘッドのいずれかを示すと思われる。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aptg0aOBjo5j"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from bertviz.transformers_neuron_view import BertModel\n",
        "from bertviz.neuron_view import show\n",
        "\n",
        "model_ckpt = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = BertModel.from_pretrained(model_ckpt)\n",
        "text = \"time flies like an arrow\"\n",
        "show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24quGMQLmEwq"
      },
      "source": [
        "- 実際にこれらをPyTorchで作成する。\n",
        "- まずはtokenizerによりテキストをトークン化する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdMfROjJmEV-"
      },
      "outputs": [],
      "source": [
        "# 単純化のため、add_special_tokens=Falseにより[CLS]や[SEP]などのトークンを除外\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "inputs.input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfr80i6jmnS4"
      },
      "source": [
        "- nn.Embeddingにより、one-hot化と密ベクトル変換を同時に行う。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRBSqPq5mtRr"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# BERTとパラメータを合わせるために、configファイルを読み込む\n",
        "config = AutoConfig.from_pretrained(model_ckpt)\n",
        "\n",
        "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "token_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dwYhIlMnWfR"
      },
      "source": [
        "- 未学習であるため初期値のままであるが、以下のように埋め込みベクトルを作成できる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voanXGgOnNad"
      },
      "outputs": [],
      "source": [
        "inputs_embeds = token_emb(inputs.input_ids)\n",
        "inputs_embeds.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S57Hc7Ggqdul"
      },
      "source": [
        "- 埋め込みを射影し、類似度をドット積を使ってアテンションスコアを計算する。\n",
        "  - 簡単のため、クエリ・キー・バリューが埋め込みベクトルと同じになっている。\n",
        "  - 実際には後述するように、それぞれ独立した重み$ W_{Q,K,V} $を適用して生成する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZfHqjS3rs-O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from math import sqrt\n",
        "\n",
        "query = key = value = inputs_embeds # 簡単のため同じものをquery, key, valueとしている。\n",
        "\n",
        "# bmmはバッチ化された行列積を計算することができる\n",
        "# sqrt(dim_k)でわるのはこれは必要なのかわからないが、\n",
        "# 埋め込みのベクトルサイズで割ることで後段のsoftmaxが飽和しないようにしているらしい。\n",
        "dim_k = key.size(-1)\n",
        "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
        "scores.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYEztnwdlBzn"
      },
      "source": [
        "- softmaxを適用する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6lpV1_Zkkre"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "weights.sum(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ06Cg8Jl518"
      },
      "source": [
        "- weightsのvalueの行列積を計算する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FQsby2dlPBD"
      },
      "outputs": [],
      "source": [
        "attn_outputs = torch.bmm(weights, value)\n",
        "attn_outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWaDoaWvmJbs"
      },
      "source": [
        "- これらを関数化すると以下となる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jclFW18fmCtv"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    dim_k = query.size(-1)\n",
        "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    return torch.bmm(weights, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYMjZKjaqXby"
      },
      "source": [
        "#### 3.2.1.2 マルチヘッドアテンション\n",
        "\n",
        "- この例ではquery, key, valueが同じベクトルであるため、その単語自身に非常に大きなスコアが割り当たる。\n",
        "- 実際は文脈を考慮するために、各埋め込みに対して独立した3つの異なる線形射影をquery, key, valueに使用する。\n",
        "- 各線形射影は学習可能なパラメータを持つ。\n",
        "- これによりセルフアテンションは系列の意味・文脈を考慮できる。\n",
        "- これらの線形射影は複数の集合を持つことで複数の関係性を考慮できるため有益である。\n",
        "- 一つ一つをアテンションヘッドといいこれらをマルチヘッドアテンションと呼ぶ。\n",
        "- 一つのヘッドのsoftmaxでは類似性の一面にしか着目できないため、複数に意味がある。\n",
        "- あるヘッドが主語と動詞の相互作用、あるヘッドが近くの形容詞を見つけるなどが可能となると考えられている。\n",
        "- マルチヘッドは最終的には図のように連結され全結合層で処理される。\n",
        "\n",
        "![](https://github.com/nokomoro3/book-ml-transformers/blob/main/img/ml-transformers-chap03-transformer-anatomy_2022-08-27-08-43-44.png?raw=1)\n",
        "\n",
        "- まずは一つのアテンションヘッドを定義する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdMeGqg0srHB"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(embed_dim, head_dim)\n",
        "        self.k = nn.Linear(embed_dim, head_dim)\n",
        "        self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attn_outputs = scaled_dot_product_attention(\n",
        "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
        "        return attn_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TuZSHvPlMBL"
      },
      "source": [
        "- head_dimは射影後の次元数で、トークン埋め込みの次元数より小さい必要はないが、\n",
        "実際にはトークン埋め込み次元数をヘッドの数で割ったものを使用する。\n",
        "- BERTの場合12個のヘッドがあるので、head_dim = 768/12 = 64となる。\n",
        "- これを複数組み合わせて以下のように定義する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uLXzu8Uk0k-"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.hidden_size\n",
        "        num_heads = config.num_attention_heads\n",
        "        head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "        x = self.output_linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyuznCcxosKu"
      },
      "source": [
        "- このクラスをテストします。\n",
        "- configからパラメータを読み込めるように定義したため、BERTのconfigを渡すことで同じ設定を使用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2WmyebEnlM2"
      },
      "outputs": [],
      "source": [
        "multihead_attn = MultiHeadAttention(config)\n",
        "attn_output = multihead_attn(inputs_embeds)\n",
        "attn_output.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFdSXFuXxHxc"
      },
      "source": [
        "- 最後にアテンションスコアの可視化をBertVizのhead_viewで行う。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRY8Ef2WpIcy"
      },
      "outputs": [],
      "source": [
        "from bertviz import head_view\n",
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n",
        "\n",
        "sentence_a = \"time flies like an arrow\"\n",
        "sentence_b = \"fruit flies like a banana\"\n",
        "\n",
        "viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt') # A,Bの文は[SEP]を挟んで連結される\n",
        "attention = model(**viz_inputs).attentions\n",
        "sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1) # UI用に文の境界を設定\n",
        "tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0]) # IDをトークンに変換\n",
        "head_view(attention, tokens, sentence_b_start, heads=[1,8]) # headsを指定すれば特定のヘッドのアテンションが確認できる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGhpqGrSzcDd"
      },
      "source": [
        "### 3.2.2 順伝搬層\n",
        "\n",
        "- 順伝搬層は2つの全結合層で構成されます。\n",
        "- 系列の位置情報は保持された状態で処理するため、一単位順伝搬層（position-wise feed-forward layer）と呼ばれる。\n",
        "- CV分野の人からはカーネルサイズ1の1次元畳み込み層と呼ばれることもある。\n",
        "- 実装としては、[batch_size, seq_len, hidden_dim]のままflattenせずに渡せば位置情報が保持される。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5jxRTQDtad4"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9qabj2B2mVx"
      },
      "source": [
        "- 文献によれば、intermediate_sizeはhidden_dimの4倍で、活性化関数はGELUが使われる。\n",
        "- intermediate_sizeはスケールアップする際に最も良くスケールされることが多い。\n",
        "- また活性化関数は1層目の後にのみ使用され、2層目の後にはdropoutが使われる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NohsrCHW2vNH"
      },
      "outputs": [],
      "source": [
        "feed_forward = FeedForward(config)\n",
        "ff_outputs = feed_forward(attn_outputs)\n",
        "ff_outputs.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbvD_xRl26Gm"
      },
      "source": [
        "### 3.2.3 レイヤー正規化の追加\n",
        "\n",
        "- レイヤ正規化には2種類ある。\n",
        "\n",
        "![](./img/ml-transformers-chap03-transformer-anatomy_2022-08-27-10-17-07.png)\n",
        "\n",
        "- レイヤー後置型は不安定であるため、学習率を徐々に上げていく学習率のウォームアップの実施が必要。\n",
        "- レイヤー前置型の方が安定かつ学習率のウォームアップが不要でありこちらを採用する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4QuVZUt2zS-"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # レイヤー正規化を適用し、入力をクエリ、キー、バリューにコピー\n",
        "        hidden_state = self.layer_norm_1(x)\n",
        "        # スキップ接続付きのアテンションを適用\n",
        "        x = x + self.attention(hidden_state)\n",
        "        # スキップ接続付きの順伝播層を適用\n",
        "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56DMshzaQtf6"
      },
      "source": [
        "- スキップ接続も実施する。スキップ接続はレイヤー正規化前の値と加算している。\n",
        "- これらを実際に処理すると以下となる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hPiJKawQ2kz"
      },
      "outputs": [],
      "source": [
        "encoder_layer = TransformerEncoderLayer(config)\n",
        "inputs_embeds.shape, encoder_layer(inputs_embeds).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqqlJiX6XWEb"
      },
      "source": [
        "- nn.LayerNormについて補足する。\n",
        "- これはBatch Normalizationの改良版で、通常バッチ方向に平均0、分散1となるようなものが、Batch Normalizationである。\n",
        "- 一方、Layer Normalizationは隠れ状態方向に平均0、分散1となるように正規化する。\n",
        "- 以下で確認してみる。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aT5qJvVV9aQ"
      },
      "outputs": [],
      "source": [
        "encoder_layer.layer_norm_1(inputs_embeds).mean(-1), encoder_layer.layer_norm_1(inputs_embeds).std(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Oq_9IvY23N"
      },
      "source": [
        "- 実際には、学習するパラメータなどやepsがあるため、正確な平均0、分散1にはなっていないがおおむね傾向を確認できる。\n",
        "- Batch Normalizationを使わない理由は、バッチサイズが小さいときに不安定になる、系列処理の場合、学習時と推論時で系列長が異なる場合があることなどが挙げられる。\n",
        "\n",
        "- ちなみに以下のGroup Normalizationの論文にあるように正確には\n",
        "  - Batch Normalizationは、画像の場合ではバッチ方向とピクセル位置方向の正規化\n",
        "  - Layer Normalizationは、画像の場合ではピクセル位置方向とチャンネル方向の正規化\n",
        "  - Instance Normalizationは、画像の場合ではピクセル位置方向のみの正規化\n",
        "\n",
        "- 以下がそれを示す図である。\n",
        "\n",
        "![](./img/ml-transformers-chap03-transformer-anatomy_2022-08-27-14-44-43.png)\n",
        "\n",
        "- この用語に沿う場合、Layer Normalizationといえるか微妙だが、nn.LayerNorm自体は任意の軸方向の正規化が可能であるため、Transformerの場合は隠れ状態方向に正規化することに使用されている。\n",
        "\n",
        "- Group Normalizationの論文は以下である。\n",
        "  - [https://arxiv.org/abs/1803.08494](https://arxiv.org/abs/1803.08494)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH2PApstWH6F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('3.9.7')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3949dca9ef5ee3d00faebebd640170151a9deedee36f4a22f588f089137dbd92"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
