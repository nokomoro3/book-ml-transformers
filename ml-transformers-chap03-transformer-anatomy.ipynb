{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの詳細\n",
    "\n",
    "- この章では、Transformerの詳細な実装について記載されている。\n",
    "- 実際にPyTorchを使って実装を進めていく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Transformerのアーキテクチャ\n",
    "\n",
    "- オリジナルのTransformerはエンコーダ・デコーダアーキテクチャとなっている。\n",
    "- もともと機械翻訳向けにこの構造をしている。\n",
    "\n",
    "<img src=\"img/ml-transformers-chap03-transformer-anatomy_2022-08-25-22-02-10.png\" />\n",
    "\n",
    "- エンコーダの特徴\n",
    "  - 入力トークン系列を埋め込みベクトルの系列に変換する。\n",
    "  - 埋め込みベクトルは、隠れ状態やコンテキストとも呼ばれる。\n",
    "- デコーダの特徴\n",
    "  - 埋め込みベクトルの系列を入力し、出力トークン系列を生成する。\n",
    "  - デコーダの終了は、特別なEOSトークンに到達するまで継続される。\n",
    "\n",
    "- その後エンコーダ・デコーダのそれぞれが独立したモデルとして適応されていくこととなる。\n",
    "\n",
    "- エンコーダのみモデル\n",
    "  - テキスト分類や固有表現認識といったタスクに使用される。\n",
    "  - このアーキテクチャでは、与えられたあるトークンの結果が、前後双方のコンテキストに依存する。\n",
    "  - これは双方向アテンションと呼ばれ、BERT系が該当する。\n",
    "- デコーダのみモデル\n",
    "  - 次の単語を文脈から予測するようなタスクに使用される。\n",
    "  - このアーキテクチャでは、与えられたあるトークンの結果が、前方のコンテキストのみに依存する。\n",
    "  - これは因果的もしくは自己回帰型アテンションと呼ばれ、GPT系が該当する。\n",
    "- エンコーダ・デコーダモデル\n",
    "  - 機械翻訳や要約といったタスクに使用される。\n",
    "  - BARTやT5がこのアーキテクチャに該当する。\n",
    "\n",
    "- 実際にはこれらの区別はあいまいであるので注意が必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 エンコーダ\n",
    "\n",
    "- Transformerのエンコーダは複数のエンコーダをスタックする。\n",
    "- エンコーダは文脈情報を埋め込んだ表現を生成する。\n",
    "- エンコーダは、マルチヘッドセルフアテンションと単純な順伝搬層で構成される。\n",
    "\n",
    "<img src=\"img/ml-transformers-chap03-transformer-anatomy_2022-08-25-22-14-41.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 セルフアテンション\n",
    "\n",
    "- アテンションは系列の各要素に異なる重みを割り当てる機構である。\n",
    "- テキストの場合、系列の要素はトークン埋め込みである。\n",
    "- トークン埋め込みは、固定次元のベクトルで表される。\n",
    "  - 例えばBERTの場合、768次元のベクトルとなる。\n",
    "- セルフアテンションは、入力されるトークン埋め込み$x_j$すべてを使用した線形和を、その系列ぶん計算する。\n",
    "\n",
    "$$ x^{'}_i = \\sum^n_{j=1}{w_{ji}x_j}$$\n",
    "\n",
    "- $w_{ji}$はアテンションの重みと呼ばれ、$\\sum_{j}{w_{ji}}=1$となるように正規化される。\n",
    "- セルフアテンションは以下のように、同一単語であっても周囲の文脈を考慮した埋め込みを生成することができる。\n",
    "\n",
    "<img src=\"img/ml-transformers-chap03-transformer-anatomy_2022-08-26-09-09-37.png\" />\n",
    "\n",
    "- この出力される結果を文脈埋め込みと呼び、Transformerに選考してELMoなどの言語モデルで取り入れられている。\n",
    "  - [Deep Contextualized Word Representations (2018-02-15)](https://arxiv.org/abs/1802.05365)\n",
    "- 以降は$w_{ji}$を計算する方法について述べる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1.1 スケール化ドット積アテンション\n",
    "\n",
    "- いくつか実装があるがTransformerでは以下のステップを踏む。\n",
    "  - 各トークン埋め込みをクエリ・キー・バリューというベクトルに射影する。\n",
    "  - クエリとキーの類似度を計算し、これをアテンションスコアと呼ぶ。\n",
    "    - アテンションスコアは、系列長をnとすると、n x n行列となる。\n",
    "  - アテンションスコアは任意の数を生成できるため、softmaxで正規化して、これを重み$ w_{ji} $とする。\n",
    "    - 正規化は入力系列方向に行い、$ \\sum_{j}{w_{ji}}=1 $となるようにする\n",
    "  - バリューベクトル$ v_1,...,v_n $の重み付け線形和$ x^{'}_i = \\sum^n_{j=1}{w_{ji}v_j} $で文脈埋め込みを計算する\n",
    "- これらの可視化をJupyter向けのBertVizにおけるneuron_viewで行うことができる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertModel.from_pretrained(model_ckpt)\n",
    "text = \"time flies like an arrow\"\n",
    "show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3949dca9ef5ee3d00faebebd640170151a9deedee36f4a22f588f089137dbd92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
