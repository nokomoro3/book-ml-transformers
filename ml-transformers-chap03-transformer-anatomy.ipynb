{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの詳細\n",
    "\n",
    "- この章では、Transformerの詳細な実装について記載されている。\n",
    "- 実際にPyTorchを使って実装を進めていく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Transformerのアーキテクチャ\n",
    "\n",
    "- オリジナルのTransformerはエンコーダ・デコーダアーキテクチャとなっている。\n",
    "- もともと機械翻訳向けにこの構造をしている。\n",
    "\n",
    "<img src=\"img/ml-transformers-chap03-transformer-anatomy_2022-08-25-22-02-10.png\" />\n",
    "\n",
    "- エンコーダの特徴\n",
    "  - 入力トークン系列を埋め込みベクトルの系列に変換する。\n",
    "  - 埋め込みベクトルは、隠れ状態やコンテキストとも呼ばれる。\n",
    "- デコーダの特徴\n",
    "  - 埋め込みベクトルの系列を入力し、出力トークン系列を生成する。\n",
    "  - デコーダの終了は、特別なEOSトークンに到達するまで継続される。\n",
    "\n",
    "- その後エンコーダ・デコーダのそれぞれが独立したモデルとして適応されていくこととなる。\n",
    "\n",
    "- エンコーダのみモデル\n",
    "  - テキスト分類や固有表現認識といったタスクに使用される。\n",
    "  - このアーキテクチャでは、与えられたあるトークンの結果が、前後双方のコンテキストに依存する。\n",
    "  - これは双方向アテンションと呼ばれ、BERT系が該当する。\n",
    "- デコーダのみモデル\n",
    "  - 次の単語を文脈から予測するようなタスクに使用される。\n",
    "  - このアーキテクチャでは、与えられたあるトークンの結果が、前方のコンテキストのみに依存する。\n",
    "  - これは因果的もしくは自己回帰型アテンションと呼ばれ、GPT系が該当する。\n",
    "- エンコーダ・デコーダモデル\n",
    "  - 機械翻訳や要約といったタスクに使用される。\n",
    "  - BARTやT5がこのアーキテクチャに該当する。\n",
    "\n",
    "- 実際にはこれらの区別はあいまいであるので注意が必要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 エンコーダ\n",
    "\n",
    "- Transformerのエンコーダは複数のエンコーダをスタックする。\n",
    "- エンコーダは文脈情報を埋め込んだ表現を生成する。\n",
    "\n",
    "<img src=\"img/ml-transformers-chap03-transformer-anatomy_2022-08-25-22-14-41.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3949dca9ef5ee3d00faebebd640170151a9deedee36f4a22f588f089137dbd92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
